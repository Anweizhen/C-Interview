### Linux基础

#### 源码编译过程

gcc 编译器驱动程序读取源文件，经过<font color=red>预处理、编译、汇编、链接</font>（分别使用预处理器、编译器、汇编器、链接器，这四个程序构成了编译系统）四个步骤，将其翻译成可执行目标程序hello。如下图所示：

![img](assets/编译过程.jpg)

**1. 预处理**（xxx.i）

主要处理源代码文件中的以“#”开头的预编译指令，生成xxx.i或xxx.ii文件。处理规则如下：

1. ==删除所有的#define，展开所有的宏定义==。
2. 处理所有的==条件预编译指令==，如“#if”、“#endif”、“#ifdef”、“#elif”和“#else”。
3. ==处理“#include”预编译指令，将文件内容替换到它的位置，这个过程是递归进行的，文件中包含其他文件==。
4. ==删除所有的注释==，“//”和“/**/”。
5. 保留所有的#pragma 编译器指令，编译器需要用到他们，如：#pragma once 是为了防止有文件被重复引用。
6. ==添加行号和文件标识==，便于编译时编译器产生调试用的行号信息，和编译时产生编译错误或警告是能够显示行号。

**2. 编译**（xxx.s）

把预编译生成的文件，进行一系列词法分析、语法分析、语义分析及优化后，生成xxx.s汇编代码文件：

1. 词法分析：利用类似于“有限状态机”的算法，将源代码程序输入到扫描机中，将其中的字符序列分割成一系列的记号：关键字、标识符、字面常量、特殊符号（括号、运算符）等
2. 语法分析：语法分析器对由扫描器产生的记号，进行语法分析，产生语法树。由语法分析器输出的语法树是一种以表达式为节点的树：如赋值表达式
3. 语义分析：语法分析器只是完成了对表达式语法层面的分析，语义分析器则对表达式是否有意义进行判断。编译器能分析的语义是静态语义。所谓静态语义是指在编译期间可以确定的语义。与之对应的动态语义是指在运行期才能确定的语义。
4. 源代码优化：源代码级别的一个优化过程。例如（2+7）会被优化，因为值在编译期间就可以确定
5. 目标代码生成：由代码生成器将中间代码转换成目标机器代码，生成一系列的代码序列——汇编语言表示。
6. 目标代码优化：目标代码优化器对上述的目标机器代码进行优化：寻找合适的寻址方式、使用位移来替代乘法运算、删除多余的指令等。

**3. 汇编**（xxx.o）

==将汇编代码转变成机器可以执行的指令(机器码文件)==。 汇编器的汇编过程相对于编译器来说更简单，没有复杂的语法，也没有语义，更不需要做指令优化，只是根据汇编指令和机器指令的对照表一一翻译过来，汇编过程有汇编器as完成。经汇编之后，产生目标文件(与可执行文件格式几乎一样)xxx.o(Windows下)、xxx.obj(Linux下)。

**4. 链接**（xxx.out）

==将不同的源文件产生的目标文件进行链接，从而形成一个可以执行的程序==。链接分为静态链接和动态链接。

1. 静态链接：

    函数和数据被编译进一个二进制文件。在使用静态库的情况下，==在编译链接可执行文件时，链接器从库中复制这些函数和数据并把它们和应用程序的其它模块组合起来创建最终的可执行文件==。

    优点：

    - 运行速度快

    缺点：

    - 空间浪费
    - 更新困难

    空间浪费：因为每个可执行程序中对所有需要的目标文件都要有一份副本，所以如果多个程序对同一个目标文件都有依赖，会出现同一个目标文件都在内存存在多个副本；

    更新困难：每当库函数的代码修改了，这个时候就需要重新进行编译链接形成可执行程序。

    运行速度快：但是静态链接的优点就是，在可执行程序中已经具备了所有执行程序所需要的任何东西，在执行的时候运行速度快。

2. 动态链接：

    动态链接的基本思想是把程序按照模块拆分成各个相对独立部分，==在程序运行时才将它们链接在一起形成一个完整的程序，而不是像静态链接一样把所有程序模块都链接成一个单独的可执行文件==。

    优点：

    - 共享库、节省空间
- 更新方便
  
    缺点：
    
    - 性能损耗
    
    共享库：就是即使需要每个程序都依赖同一个库，但是该库不会像静态链接那样在内存中存在多分，副本，而是这多个程序在执行时共享同一份副本；
    
    更新方便：更新时只需要替换原来的目标文件，而无需将所有的程序再重新链接一遍。当程序下一次运行时，新版本的目标文件会被自动加载到内存并且链接起来，程序就完成了升级的目标。                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        
    
    性能损耗：因为把链接推迟到了程序运行时，所以每次执行程序都需要进行链接，所以性能会有一定损失。



#### 内核态和用户态

**地址空间**

linux进程有4GB地址空间，如图所示：

![img](assets/20180721092710523.png)

3G-4G（高地址空间）大部分是共享的，是内核态的地址空间。这里存放整个内核的代码和所有的内核模块以及内核所维护的数据。

**用户态和内核态**

<font color=red>当一个进程在执行用户自己的代码时处于用户运行态（用户态）</font>，此时特权级最低，为3级，是普通的用户进程运行的特权级，大部分用户直接面对的程序都是运行在用户态。Ring3状态不能访问Ring0的地址空间，包括代码和数据；<font color=red>当一个进程因为系统调用陷入内核代码中执行时处于内核运行态（内核态）</font>，此时特权级最高，为0级。执行的内核代码会使用当前进程的内核栈，每个进程都有自己的内核栈。

用户运行一个程序，该程序创建的进程开始时运行自己的代码，处于用户态。<font color=red>如果要执行文件操作、网络数据发送等操作必须通过write()、send()等系统调用，这些系统调用会调用内核的代码</font>。进程会切换到Ring0，然后进入3G-4G中的内核地址空间去执行内核代码来完成相应的操作。内核态的进程执行完后又会切换到Ring3，回到用户态。这样，用户态的程序就不能随意操作内核地址空间，具有一定的安全保护作用。这里说的保护模式是指通过内存页表操作等机制，保证进程间的地址空间不会互相冲突，一个进程的操作不会修改另一个进程地址空间中的数据。

> 为什么要划分用户空间和系统空间呢？
>
> 当然是有必要的。操作系统的数据都是存放于系统空间的，用户进程的数据是存放于用户空间的。这是第一点，不同的身份，数据放置的位置必然不一样，否则大混战就会导致系统的数据和用户的数据混在一起，系统就不能很好的运行了。分开来存放，就让系统的数据和用户的数据互不干扰，保证系统的稳定性。分开存放，管理上很方便，而更重要的是，将用户的数据和系统的数据隔离开，就可以对两部分的数据的访问进行控制。这样就可以确保用户程序不能随便操作系统的数据，这样防止用户程序误操作或者是恶意破坏系统。

**概念**

- <font color=red>当一个进程在执行用户自己的代码时处于用户运行态（用户态）</font>

- <font color=red>当一个进程因为系统调用陷入内核代码中执行时处于内核运行态（内核态）</font>

**区别：**

==用户态和内核态是操作系统的两种运行级别==，<font color=red>两者最大的区别就是特权级不同</font>。用户态拥有最低的特权级，内核态拥有较高的特权级。运行在用户态的程序不能直接访问操作系统内核数据结构和程序。

**用户态和内核态的切换**

当在系统中执行一个程序时，大部分时间是运行在用户态下的，在其需要操作系统帮助完成一些用户态自己没有特权和能力完成的操作时就会切换到内核态。

用户态切换到内核态的3种方式：

- （1）系统调用

    这是用户态进程主动要求切换到内核态的一种方式。用户态进程通过系统调用申请使用操作系统提供的服务程序完成工作。例如fork())就是执行了一个创建新进程的系统调用。系统调用的机制和新是使用了操作系统为用户特别开放的一个中断来实现，如Linux的int 80h中断。

- （2）异常

    当cpu在执行运行在用户态下的程序时，发生了一些没有预知的异常，这时会触发由当前运行进程切换到处理此异常的内核相关进程中，也就是切换到了内核态，==如缺页异常==。

- （3）外围设备的中断

    当外围设备完成用户请求的操作后，会向CPU发出相应的中断信号，这时CPU会暂停执行下一条即将要执行的指令而转到与中断信号对应的处理程序去执行，如果前面执行的指令时用户态下的程序，那么转换的过程自然就会是 由用户态到内核态的切换。如硬盘读写操作完成，系统会切换到硬盘读写的中断处理程序中执行后边的操作等。

这三种方式是系统在运行时由用户态切换到内核态的最主要方式，其中系统调用可以认为是用户进程主动发起的，异常和外围设备中断则是被动的。从触发方式上看，切换方式都不一样，但从最终实际完成由用户态到内核态的切换操作来看，步骤有事一样的，都相当于执行了一个中断响应的过程。系统调用实际上最终是中断机制实现的，而异常和中断的处理机制基本一致。

**产生原因：**

> 操作系统为什么要分内核态和用户态

为了安全性。在cpu的一些指令中，有的指令如果用错，将会导致整个系统崩溃。分了内核态和用户态后，当用户需要操作这些指令时候，内核为其提供了API，可以通过系统调用进入内核，让内核去执行这些操作。

`注意：这部分的内容主要是帮助理解epoll、poll、select工作原理`

> [内核态和用户态](https://blog.csdn.net/qq_39823627/article/details/78736650)

#### 系统调用

==操作系统提供一系列具备预定功能的内核函数， 通过一组称为系统调用（system call)的接口呈现给用户。系统调用把用户进程的请求传达给内核 ，调用相应的内核函数完成所需的处理 ，再将处理结果返回给用户空间。== 

计算机中，系统调用（英语：system call），又称为系统呼叫，指运行在用户空间的程序向操作系统内核请求需要更高权限运行的服务。系统调用提供了用户程序与操作系统之间的接口（即==系统调用是用户程序和内核交互的接口==）。

操作系统中的状态分为管态（核心态）和目态（用户态）。大多数系统交互式操作需求在内核态执行。如设备IO操作或者进程间通信。特权指令：一类只能在核心态下运行而不能在用户态下运行的特殊指令。不同的操作系统特权指令会有所差异，但是一般来说主要是和硬件相关的一些指令。用户程序只在用户态下运行，有时需要访问系统核心功能，这时通过系统调用接口使用系统调用。

应用程序有时会需要一些危险的、权限很高的指令，如果把这些权限放心地交给用户程序是很危险的(比如一个进程可能修改另一个进程的内存区，导致其不能运行)，但是又不能完全不给这些权限。于是有了系统调用，危险的指令被包装成系统调用，用户程序只能调用而无权自己运行那些危险的指令。另外，计算机硬件的资源是有限的，为了更好的管理这些资源，所有的资源都由操作系统控制，进程只能向操作系统请求这些资源。操作系统是这些资源的唯一入口，这个入口就是系统调用。

##### 系统调用举例

对文件进行写操作，程序向打开的文件写入字符串“hello world”，open和write都是系统调用。如下：

```
#include<stdio.h>
#include<stdlib.h>
#include<string.h>
#include<errno.h>
#include<unistd.h>
#include<sys/types.h>
#include<sys/stat.h>
#include<fcntl.h>
int main(int argc, char *argv[])
{
    if (argc<2)
        return 0;
    //用读写追加方式打开一个已经存在的文件
    int fd = open(argv[1], O_RDWR | O_APPEND);
    if (fd == -1)
    {
        printf("error is %s\n", strerror(errno));
    }
    else
    {
        //打印文件描述符号
        printf("success fd = %d\n", fd);
        char buf[100];
        memset(buf, 0, sizeof(buf));
        strcpy(buf, "hello world\n");
        write(fd, buf, strlen(buf));
        close(fd);
    }
    return 0;
}
```

还有写数据write，创建进程fork，vfork等都是系统调用。

常用系统调用：

- ==文件读写的系统调用==
- open 打开文件
    - creat 创建新文件
    - close 关闭文件描述字
    - read 读文件
    - write 写文件
    - [pread](https://baike.baidu.com/item/pread) 对文件随机读
    - [pwrite](https://baike.baidu.com/item/pwrite) 对文件随机写
    - lseek 移动文件指针
- ==进程控制的系统调用==
- fork 创建一个新进程
    - clone 按指定条件创建子进程
- exit 中止进程
    - wait父进程 等待子进程终止
    -  getpid 获取进程标识号 
- ==文件操作的系统调用==
- mkdir 创建目录
  
- rmdir 删除目录
  
- rename 文件改名
    - stat 取文件状态信息
    - chmod 改变文件方式
- 系统控制

    -  time 取得系统时间 
    -  getitimer 获取[计时器](https://baike.baidu.com/item/计时器)值 
    - reboot 重新启动
- 内存管理

    - brk 改变数据段空间的分配
    - mmap 映射虚拟内存页
    -  msync 将映射内存中的数据写回磁盘 
- 用户管理
    - 
- socket套接字
    - socket 建立socket
    - bind 绑定socket到端口
    - connect 连接远程主机
    - accept 响应socket连接请求
    - send 通过socket发送信息
- 进程间通信


##### 补充：

系统调用概述：

现代操作系统为了保证安全性，通常把运行状态分为用户态和内核态，并规定，只有特权指令才能在内核态运行，用户程序只能在用户态下运行。那么，假如用户程序需要访问系统的核心功能，怎么办？没错儿，就是通过系统调用。

==简单来说，系统调用就是把应用程序的请求传递给内核，调用相应的内核函数来处理请求，然后再将处理结果返回给应用程序==。

本次实验选用 `Linux` 操作系统 2 号系统调用，`fork` 函数，通过对比嵌入式汇编编程和系统函数编程，深入理解和分析系统调用的过程。

> https://www.jianshu.com/p/0e25bee35c66

总结

本实验通过分析 Linux 内核系统调用源码，深入剖析了系统调用的全过程。==通过 `int 0x80` 中断跳转到系统调用处理程序 `system_call` 函数处，执行相应的例程==。

但是，由于是代表的是用户进程，所以这个执行过程并不属于中断上下文，而是进程上下文。因此，系统调用执行过程中，可以访问用户进程的许多信息，也可以被更高优先级的进程抢占。

当系统调用完成后，把控制权交回到发起调用的用户进程前，内核又会有一次调度。如果发现有优先级更高的进程或当前进程的时间片用完，那么会选择优先级更高的进程或重新选择进程执行。

> https://www.jianshu.com/p/27a5ad4d636b

系统调用系统调用就是用户空间应用程序和内核提供的服务之间的一个接口。由于服务是在内核中提供的，因此无法执行直接调用；相反，我们必须使用一个进程来跨越用户空间与内核之间的界限，这实际上就是系统调用的过程。我们首先来开一个系统调用流程的示意图。

![img](assets/021332021392029-1568260966100.png)

整个过程如下：

- 首先指令流执行到系统调用函数时，==系统调用函数通过int 0x80指令进入系统调用入口程序==，并且把系统调用号放入%eax中，如果需要传递参数，则把参数放入%ebx，%ecx和%edx中。
- 进入系统调用入口程序（System_call）后，它首先把相关的寄存器压入内核堆栈（以备将来恢复），这个过程称为保护现场。保护现场的工作完成后，开始检查系统调用号是不是一个有效值，如果不是则退出。
- 接下来根据系统调用号开始调用系统调用处理程序（这是一个正式执行系统调用功能的函数），从系统调用处理程序返回后，就会去检查当前进程是否处于就绪态、进程时间片是否用完，如果不在就绪态或者时间片已用完，那么就会去调用进程调度程序schedule()，转去执行其他进程。
- 如果不执行进程调度程序，那么接下来就会开始执行ret_from_sys_call，顾名思义，这这个程序主要执行一些系统调用的后处理工作。比如它会去检查当前进程是否有需要处理的信号，如果有则去调用do_signal()，然后进行一些恢复现场的工作，返回到原先的进程指令流中。
- 至此整个系统调用的过程就结束了。

> [系统调用过程详解](https://blog.csdn.net/weixin_30642267/article/details/97343791)
>
> [系统调用的过程](https://blog.csdn.net/chengonghao/article/details/51288084)



#### 缺页中断

malloc()和mmap()等内存分配函数，在分配时只是建立了进程虚拟地址空间，并没有分配虚拟内存对应的物理内存。当进程访问这些没有建立映射关系的虚拟内存时，处理器自动触发一个缺页异常。

缺页中断：在请求分页系统中，可以通过查询页表中的状态位来确定所要访问的页面是否存在于内存中。每当所要访问的页面不在内存是，会产生一次缺页中断，此时操作系统会根据页表中的外存地址在外存中找到所缺的一页，将其调入内存。

缺页本身是一种中断，与一般的中断一样，需要经过4个处理步骤：

1、保护CPU现场

2、分析中断原因

3、转入缺页中断处理程序进行处理

4、恢复CPU现场，继续执行

但是缺页中断是由于所要访问的页面不存在于内存时，由硬件所产生的一种特殊的中断，因此，与一般的中断存在区别：

1、在指令执行期间产生和处理缺页中断信号

2、一条指令在执行期间，可能产生多次缺页中断

3、缺页中断返回是，执行产生中断的一条指令，而一般的中断返回是，执行下一条指令。

#### 锁机制

> 说一说Linux的4种锁机制

互斥锁：mutex，用于保证在任何时刻，都只能有一个线程访问该对象。当获取锁操作失败时，线程会进入睡眠，等待锁释放时被唤醒

读写锁：rwlock，分为读锁和写锁。处于读操作时，可以允许多个线程同时获得读操作。但是同一时刻只能有一个线程可以获得写锁。其它获取写锁失败的线程都会进入睡眠状态，直到写锁释放时被唤醒。 注意：写锁会阻塞其它读写锁。当有一个线程获得写锁在写时，读锁也不能被其它线程获取；写者优先于读者（一旦有写者，则后续读者必须等待，唤醒时优先考虑写者）。适用于读取数据的频率远远大于写数据的频率的场合。

自旋锁：spinlock，在任何时刻同样只能有一个线程访问对象。但是当获取锁操作失败时，不会进入睡眠，而是会在原地自旋，直到锁被释放。这样==节省了线程从睡眠状态到被唤醒期间的消耗，在加锁时间短暂的环境下会极大的提高效率。但如果加锁时间过长，则会非常浪费CPU资源==。

RCU：即read-copy-update，在修改数据时，首先需要读取数据，然后生成一个副本，对副本进行修改。修改完成后，再将老数据update成新的数据。使用RCU时，读者几乎不需要同步开销，既不需要获得锁，也不使用原子指令，不会导致锁竞争，因此就不用考虑死锁问题了。而对于写者的同步开销较大，它需要复制被修改的数据，还必须使用锁机制同步并行其它写者的修改操作。在有大量读操作，少量写操作的情况下效率非常高。

[RCU的原理](https://www.cnblogs.com/cobbliu/p/4888225.html)

RCU(Read-Copy Update)，顾名思义就是读-拷贝修改，它是基于其原理命名的。对于被RCU保护的共享数据结构，读者不需要获得任何锁就可以访问它，但写者在访问它时首先拷贝一个副本，然后对副本进行修改，最后使用一个回调（callback）机制在适当的时机把指向原来数据的指针重新指向新的被修改的数据。这个时机就是所有引用该数据的CPU都退出对共享数据的操作。

因此RCU实际上是一种改进的rwlock。

####　一致性哈希

[白话解析：一致性哈希算法](http://www.zsythink.net/archives/1182/) 讲解的很好

[面试必备：什么是一致性Hash算法？](https://blog.csdn.net/bntX2jSQfEHy7/article/details/79549368)

[一致性哈希算法原理](https://www.cnblogs.com/williamjie/p/9477852.html)

[三分钟看懂一致性哈希算法](https://blog.csdn.net/gerryke/article/details/53939212)

考虑到分布式系统每个节点都有可能失效，并且新的节点很可能动态的增加进来，如何保证当系统的节点数目发生变化时仍然能够对外提供良好的服务，这是值得考虑的，尤其实在设计分布式缓存系统时，如果某台服务器失效，对于整个系统来说如果不采用合适的算法来保证一致性，那么缓存于系统中的所有数据都可能会失效（即由于系统节点数目变少，客户端在请求某一对象时需要重新计算其hash值（通常与系统中的节点数目有关），由于hash值已经改变，所以很可能找不到保存该对象的服务器节点），因此一致性hash就显得至关重要，良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面：

- **平衡性(Balance)**

    平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。

- **单调性(Monotonicity)**

    单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。简单的哈希算法往往不能满足单调性的要求，如最简单的线性哈希：x = (ax + b) mod (P)，在上式中，P表示全部缓冲的大小。不难看出，当缓冲大小发生变化时(从P1到P2)，原来所有的哈希结果均会发生变化，从而不满足单调性的要求。哈希结果的变化意味着当缓冲空间发生变化时，所有的映射关系需要在系统内全部更新。而在P2P系统内，缓冲的变化等价于Peer加入或退出系统，这一情况在P2P系统中会频繁发生，因此会带来极大计算和传输负荷。单调性就是要求哈希算法能够应对这种情况。

- **分散性(Spread)**

    在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。

- **负载(Load)**

    负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。

- **平滑性(Smoothness)**

    平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。

均衡性：哈希的结果能够尽可能分布到所有的缓存中去

单调性：当缓冲区大小变化时一致性哈希尽量保护已分配的内容不会被重新映射到新缓冲区

分散性：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。

负载：另一个维度的分散性问题，对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。

-----------------------------------------------------------

### Linux常用命令

参考《Linux常用命令》

### 内存

#### 内存布局

在编写程序时使用的内存空间叫虚拟内存，程序在运行时，要完成虚拟内存到物理内存的转换。假如在32位环境上，理论上我们可以使用的虚拟内存空间是4GB，但实际上这4GB并不能完全给我们使用，有一部分内存是固定分配给操作系统内核的，分配给操作系统的这部分内存我们叫它内核空间，除去内核空间以后用户能使用的内存叫用户空间，在linux 32环境下，高地址的1GB（0xc00000000—0xffffffff）是固定给内核使用的，低地址的3GB才是给用户使用的。

![img](assets/1061105-20190502135205381-1624508314.png)

用户空间内存分配情况：

- **栈区**（stack）：向下增长

    由**编译器自动分配和释放**的，存放函数的参数值、局部变量的值等。操作方式类似数据结构中的栈。在函数参数和地址入栈情况中，一般是从左向右将变量入栈，最后函数的地址入栈。

- **堆区**（heap）/自由存储区（free store）：向上增长

    由程序员**手动分配和释放**，完全**不同于数据结构中的堆**，==分配方式类似链表==。即由malloc或new来分配，free和delete释放。

- **全局数据区/静态存储区**

    全局变量和静态变量的内存是存储放在一块的，分为两块：

    - 未初始化的全局变量和未初始化的静态变量在相邻的另一块区域；
    - 初始化的全局变量和静态变量在一块区域；

    程序结束后由系统释放。（有读写权限，值可以改变）

- **常量区**

    一般常量、字符串常量存放在这里，程序结束时系统释放。（只有读取权限，没有写入权限，因此它们的值在程序运行期间不能改变。）

- **程序代码区**

    存放函数体的二进制代码。

> [linux下C/C++程序的内存布局](https://www.cnblogs.com/418ks/p/10802184.html)

2、A* a = new A; a->i = 10：

1）A *a：a是一个局部变量，类型为指针，故而操作系统在程序栈区开辟4/8字节的空间（0x000m），分配给指针a。

2）new A：通过new动态的在堆区申请类A大小的空间（0x000n）。

3）a = new A：将指针a的内存区域填入栈中类A申请到的地址的地址。即*（0x000m）=0x000n。

4）a->i：先找到指针a的地址0x000m，通过a的值0x000n和i在类a中偏移offset，得到a->i的地址0x000n + offset，进行*(0x000n + offset) = 10的赋值操作，即内存0x000n + offset的值是10。

#### 内存对齐

**除了\*（即指针变量）与long随操作系统子长变化而变化外，其他的都固定不变(32位和64相比)**

- 32位系统
    - *(即指针变量)： 4个字节
    - long：4个字节
- 64位系统
    - *(即指针变量)：8个字节
    - long：8个字节

实际测试：

```cpp
//内存对齐（memory alignment）

#include<iostream>

using namespace std;

class test
{
	char c;
	int i;
	long long s;
};

class alignas(16) test1
{
	char c;
	int i;
	long long s;
};

class test2
{
	char c;
	long long s;
	int i;
};

class alignas(16) test3
{
	char c;
	long long s;
	int i;
};

class alignas(long long) test4
{
	char c;
	long long s;
	int i;
};

int main()
{
	char a;
	short b;
	int c;
	long d;
	long long e;
	
	cout << "char  = " << sizeof(a) << endl;
	cout << "short = " << sizeof(b) << endl;
	cout << "int   = " << sizeof(c) << endl;
	cout << "long  = " << sizeof(d) << endl;
	cout << "llong = " << sizeof(e) << endl;

	cout << "\n*****Test*****" << endl;
	cout << "test  = " << sizeof(test)  << endl;   // 16
	cout << "test1 = " << sizeof(test1) << endl;   // 16，说明alignas(16)对内存分配无影响
	cout << "test2 = " << sizeof(test2) << endl;   // 24，说明变量顺序对内存分配有影响
	cout << "test3 = " << sizeof(test3) << endl;   // 32，说明alignas(16)对内存分配有影响
	cout << "test4 = " << sizeof(test4) << endl;   // 24，说明变量顺序对内存分配有影响
}
```

结果：

```cpp
char  = 1
short = 2
int   = 4
long  = 4  //32位系统
llong = 8

*****Test*****
test  = 16
test1 = 16
test2 = 24
test3 = 32
test4 = 24
```

结论:

- Test说明char和int共享8个bit

- Test2说明char和int独享8个bit

- Test和Test2说明，内存对齐和变量的大小及顺序有关

    ![1570239596648](面试问题汇总及答案_操作系统.assets/1570239596648.png)

- test3说明alignas(n)可以调整对齐规则，但是只是最大的变量按照n来调整了

#### 内存管理



#### 虚拟内存

为了更加有效的管理内存并少出错，现代系统提供了一种对主存的抽象的概念，叫做**虚拟内存(VM)**。

`虚拟内存`是硬件异常、硬件地址翻译、主存、磁盘文件和内核软件间的完美交互，为每个进程提供了一个大的、一致的和私有的地址空间。

虚拟内存提供了三个重要的能力： **缓存**，**内存管理**，**内存保护**

1. 将主存视为一个存储在磁盘上的地址空间的高速缓存，在主存中只保存活动区域，并根据需要在磁盘和主存之间来回传送数据

2. 为每个进程提供了一致的地址空间，简化内存管理 P565

3. 保护了每个进程的地址空间不被其他进程破坏 P567

**为什么使用虚拟内存**



#### 虚拟内存

为了防止不同进程同一时刻在物理内存中运行而对物理内存的争夺和践踏，采用了虚拟内存。
虚拟内存技术使得不同进程在运行过程中，它所看到的是自己独自占有了当前系统的4G内存。所有进程共享同一物理内存，每个进程只把自己目前需要的虚拟内存空间映射并存储到物理内存上。 事实上，在每个进程创建加载时，内核只是为进程“创建”了虚拟内存的布局，具体就是初始化进程控制表中内存相关的链表，实际上并不立即就把虚拟内存对应位置的程序数据和代码（比如.text .data段）拷贝到物理内存中，只是建立好虚拟内存和磁盘文件之间的映射就好（叫做存储器映射），等到运行到对应的程序时，才会通过缺页异常，来拷贝数据。还有进程运行过程中，要动态分配内存，比如malloc时，也只是分配了虚拟内存，即为这块虚拟内存对应的页表项做相应设置，当进程真正访问到此数据时，才引发缺页异常。

请求分页系统、请求分段系统和请求段页式系统都是针对虚拟内存的，通过请求实现内存与外存的信息置换。


虚拟内存的好处：

1.扩大地址空间；

2.内存保护：每个进程运行在各自的虚拟内存地址空间，互相不能干扰对方。虚存还对特定的内存地址提供写保护，可以防止代码或数据被恶意篡改。

3.公平内存分配。采用了虚存之后，每个进程都相当于有同样大小的虚存空间。

4.当进程通信时，可采用虚存共享的方式实现。

5.当不同的进程使用同样的代码时，比如库文件中的代码，物理内存中可以只存储一份这样的代码，不同的进程只需要把自己的虚拟内存映射过去就可以了，节省内存

6.虚拟内存很适合在多道程序设计系统中使用，许多程序的片段同时保存在内存中。当一个程序等待它的一部分读入内存时，可以把CPU交给另一个进程使用。在内存中可以保留多个进程，系统并发度提高

7.在程序需要分配连续的内存空间的时候，只需要在虚拟内存空间分配连续空间，而不需要实际物理内存的连续空间，可以利用碎片


虚拟内存的代价：

1.虚存的管理需要建立很多数据结构，这些数据结构要占用额外的内存

2.虚拟地址到物理地址的转换，增加了指令的执行时间。

3.页面的换入换出需要磁盘I/O，这是很耗时的

4.如果一页中只有一部分数据，会浪费内存。



### 死锁

> 什么是死锁？产生原因？死锁产生的条件？以及如何解决死锁？

#### 死锁

 在两个或者多个并发进程中，如果每个进程持有某种资源而又等待其它进程释放它或它们现在保持着的资源，在未改变这种状态之前都不能向前推进，称这一组进程产生了死锁。通俗的讲就是两个或多个进程在执行的过程中，因争夺资源而造成的相互等待的现象。

#### ==产生原因==

- 系统资源不足
- 资源分配不当
- 进程运行推进顺序不合适

#### ==产生条件==

死锁产生的四个条件（有一个条件不成立，则不会产生死锁）：

- 互斥条件：一个资源一次只能被一个进程使用
- 请求与保持条件：一个进程因请求资源而阻塞时，对已获得资源保持不放
- 不可剥夺条件：进程获得的资源，在未完全使用完之前，不能强行剥夺
- 环路等待条件：若干进程之间形成一种头尾相接的环形等待资源关系 

####  处理策略

 为使系统不发生死锁，必须设法破坏产生死锁的四个必要条件之一，或者允许死锁产生，但当死锁发生时能检测出思索，并有能力实现恢复。
 一般有死锁的预防、死锁避免、死锁检测、死锁解除：

- 死锁预防：

    破坏导致死锁必要条件中的任意一个就可以预防死锁

- 死锁避免

    避免是指进程在每次申请资源时判断这些操作是否安全，例如，使用银行家算法。死锁避免算法的执行会增加系统的开销

- 死锁检测：

    死锁预防和避免都是事前措施，而死锁的检测则是判断系统是否处于死锁状态，如果是，则执行死锁解除策略

- 死锁解除

    这是与死锁检测结合使用的，它使用的方式就是剥夺。即将某进程所拥有的资源强行收回，分配给其他的进程

#### ==死锁预防==

 解决死锁的方法即破坏上述四个条件中的任意一个，主要方法如下：

- 破坏互斥条件

    把独占资源变为共享资源（有些资源不允许如打印机，因此该方法不实用）

- 破坏请求和保持条件

    资源一次性分配即预先分配策略（一次性的请求所有需要的资源，并且阻塞这个进程直到所有请求都同时满足。这个方法比较低效）

- 破坏不可剥夺条件

    即当进程新的资源未得到满足时，释放已占有的资源，从而破坏不可剥夺的条件

- 破坏环路等待条件

    资源有序分配法：系统给每类资源赋予一个序号，所有进程只能采用按序号递增的形式请求资源，释放则相反，从而破坏环路等待的条件

```
#include<iostream>
#include<string>
using namespace std;
int main()
{
     int n;
     cin >> n;//n表示你要处理多少组
     string a;//定义字符串

     for (int i = 0; i < n; i++)
     {
          int num=0;
         cin >> a;
          bool b[50] = { 0 };
          for (int j = 0; j < a.size(); j++)
          {
               if (b[a[j] - 'a'] == 0)
               {
                    //cout << a[j];
                    num++;
                    b[a[j] - 'a'] = 1;//重复字符-'a'总是等于相同值，故重复字符对应的布尔值都为1；
               }
          }
         cout << num << endl;
      }
}
```



#### 死锁避免

在系统运行过程中，对进程发出的每一个系统能够满足的资源申请进行动态检查，并根据检查结果决定是否分配资源，若分配后系统发生死锁或可能发生死锁，则不予分配，否则予以分配。

- 进程启动拒绝

    如果一个进程的请求会导致死锁，则不启动该进程

- 资源分配拒绝

    如果一个进程增加的资源请求会导致死锁，则不允许此分配（银行家算法：仿照银行发放贷款时采取的控制方式而设计的一种死锁避免算法）

#### 死锁检测

允许死锁发生，但是操作系统会不断监视系统进展情况，判断死锁是否真的发生

 ~~检测时机：~~

- ~~当进程由于资源请求不满足而等待时检测死锁。缺点：系统开销大~~
- ~~定时检测~~
- ~~系统资源利用率下降时检测死锁~~

 #### 死锁解除

 重要的是以最小的代价恢复系统的运行：

- 重新启动

    代价太大

- 剥夺资源

    从其它进程剥夺足够数量的资源给死锁进程，以解除死锁状态

- 撤消进程

    最简单的撤消进程的方法是使全部死锁进程都夭折掉；稍微温和一点的方法是按照某种顺序逐个地撤消进程，直至有足够的资源可用，使死锁状态消除为止。

- 进程回退

    让一个或多个进程回退到足以回避死锁的地步，进程回退时资源释放资源而不是被剥夺。要求系统保持进程的历史信息，设置还原点。

> [死锁及解决办法](https://blog.csdn.net/abigale1011/article/details/6450845)
>
> [死锁的原因、条件和解决办法](https://www.jianshu.com/p/26881a1b9e30)
>
> [死锁](https://blog.csdn.net/rabbit_in_android/article/details/50530960)

------------------------------------------------------------------------------

### 进程和线程

#### ==线程和进程==

进程是对运行时程序的封装，是系统进行资源调度和分配的的基本单位，实现了操作系统的并发；

线程是进程的子任务，是CPU调度和分派的基本单位，用于保证程序的实时性，实现进程内部的并发；<font color=red>线程是操作系统可识别的最小执行和调度单位</font>。每个线程都独自占用一个虚拟处理器：独自的寄存器组，指令计数器和处理器状态。每个线程完成不同的任务，但是共享同一地址空间（也就是同样的动态内存，映射文件，目标代码等等），打开的文件队列和其他内核资源。

==两者区别==：

- （1）依赖关系：

    <font color=red>一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程。线程依赖于进程而存在；</font>

- （2）资源分配：

    <font color=red>进程在执行过程中拥有独立的内存单元，而多个线程共享进程的内存。</font>（资源分配给进程，同一进程的所有线程共享该进程的所有资源。同一进程中的多个线程共享代码段（代码和常量），数据段（全局变量和静态变量），扩展段（堆存储）。但是每个线程拥有自己的栈段，栈段又叫运行时段，用来存放所有局部变量和临时变量。）

- （3）调度方面：

    进程是资源分配的最小单位，线程是CPU调度的最小单位；

    同一进程的线程切换不会引起进程切换，不同进程中线程切换会引起进程切换；

- （4）系统开销（创建/撤销/切换）：

    由于在创建或撤消进程时，系统都要为之分配或回收资源，如内存空间、I／o设备等。因此，操作系统所付出的开销将显著地大于在创建或撤消线程时的开销。类似地，在进行进程切换时，涉及到整个当前进程CPU环境的保存以及新被调度运行的进程的CPU环境的设置。

    线程切换只须保存和设置少量寄存器的内容，并不涉及存储器管理方面的操作。可见，进程切换的开销也远大于线程切换的开销；

- （5）通信/同步机制：

    由于同一进程中的多个线程具有相同的地址空间，致使它们之间的同步和通信的实现，也变得比较容易。进程间通信IPC，线程间可以直接读写进程数据段（如全局变量）来进行通信——需要进程同步和互斥手段的辅助，以保证数据的一致性。在有的系统中，线程的切换、同步和通信都无须操作系统内核的干预

- （6）相互影响：

    进程间不会相互影响 ；线程一个线程挂掉将导致整个进程挂掉

- （7）可靠性：

    进程编程调试简单可靠性高，但是创建销毁开销大；线程正相反，开销小，切换速度快，但是编程调试相对复杂；

> [操作系统（一）](https://www.nowcoder.com/tutorial/93/156e55e0579d4a678e857b34d572c278)

#### 进程切换（==待完善==）

> 【1】进程切换需要保存什么信息，线程切换需要保存什么信息？

> 进程切/线程切换保存的信息：我回答的是首先肯定涉及到CPU上下文切换，所以进程切换以及线程切换都需要保存CPU的上下文（CPU寄存器和程序计数器）。在此基础上，进程切换需要保存进程的私有资源（如虚拟内存），线程切换（一般认为是同一进程的两个线程）需要保存线程间的私有资源（如线程栈）

-  进程切换时，涉及到整个当前进程CPU环境的保存以及新被调度运行的进程的CPU环境的设置。

- 线程切换时，只须保存和设置少量寄存器的内容，并不涉及存储器管理方面的操作

> 【2】线程切换需要保存哪些上下文，SP、PC、EAX这些寄存器是什么用的

 线程在切换的过程中需要保存当前线程Id、线程状态、堆栈、寄存器状态等信息。其中寄存器主要包括SP PC EAX等寄存器，其主要功能如下：

- SP：堆栈指针，指向当前栈的栈顶地址
- PC：程序计数器，存储下一条将要执行的指令
- EAX：累加寄存器，用于加法乘法的缺省寄存器

#### 有了进程，为什么还要线程

线程产生的原因：

进程可以使多个程序能并发执行，以提高资源的利用率和系统的吞吐量。但是其具有一些缺点：

- 进程在同一时间只能干一件事；
- 进程在执行的过程中如果阻塞，整个进程就会挂起，即使进程中有些工作不依赖于等待的资源，仍然不会执行；

因此，操作系统引入了比进程粒度更小的线程，作为并发执行的基本单位，从而减少程序在并发执行时所付出的时空开销，提高并发性。

和进程相比，线程的优势（其实也就是和进程的区别）如下：

- 从资源上来讲，线程是一种非常"节俭"的多任务操作方式。在linux系统下，启动一个新的进程必须分配给它独立的地址空间，建立众多的数据表来维护它的代码段、堆栈段和数据段，这是一种"昂贵"的多任务工作方式。

- 从切换效率上来讲，运行于一个进程中的多个线程，它们之间使用相同的地址空间，而且线程间彼此切换所需时间也远远小于进程间切换所需要的时间。据统计，一个进程的开销大约是一个线程开销的30倍左右。

- 从通信机制上来讲，线程间方便的通信机制。对不同进程来说，它们具有独立的数据空间，要进行数据的传递只能通过进程间通信的方式进行，这种方式不仅费时，而且很不方便。线程则不然，由于同一进城下的线程之间贡献数据空间，所以一个线程的数据可以直接为其他线程所用，这不仅快捷，而且方便。

除以上优点外，多线程程序作为一种多任务、并发的工作方式，还有如下优点：

- 1、使多CPU系统更加有效。操作系统会保证当线程数不大于CPU数目时，不同的线程运行于不同的CPU上。
- 2、改善程序结构。一个既长又复杂的进程可以考虑分为多个线程，成为几个独立或半独立的运行部分，这样的程序才会利于理解和修改。

#### 进程通信/同步

`字节跳动`

进程间的通信包括管道，命名管道，消息队列，共享内存等。一般管道比较慢，消息队列做同步，共享内存快。

1. 管道（pipe）

    半双工、数据只能单向流动、而且只能在具有血缘关系的进程间使用（父子进程、兄弟进程）

2. 命名管道（named pipe）

    也是半双工的通信方式，但是它允许无亲缘关系进程间通信

3. 信号（signal）

    信号是一种比较复杂的通信方式，用于通知接收进程某一事件已经发生

4. 消息队列（message queue）

    消息队列是由消息组成的链表，存放在内核中并由消息队列标识符标识

5. 共享内存

    共享内存是进程间最快的通信方式，这段共享内存由一个进程创建，但是多个进程可以访问，但是其本身没有提供同步机制。

6. 信号量

    可以用来控制多个进程对共享资源的访问，通常作为一种锁机制，主要用来解决同步问题。（也可以用于线程的同步）

    <font color=red>用信号量解决共享内存的同步问题：《后台开发》P<sub>369</sub> 【例11.5】</font>

7. 套接字（socket)

    套接字是一种双向通信机制。

> 《后台开发：核心技术与应用实践》
>
> [进程间通信IPC (InterProcess Communication)](https://www.jianshu.com/p/c1015f5ffa74)
>
> [进程间的几种通信方式的比较和线程间的几种通信方式](https://blog.csdn.net/yang_teng_/article/details/53325280)

#### ==线程通信同步==

- 1、临界区：==(Windows????)==

    通过多线程的串行化来访问公共资源或一段代码，速度快，适合控制数据访问；

    临界区是一段独占对某些共享资源访问的代码，在任意时刻只允许一个线程对共享资源进行访问。如果有多个线程试图同时访问临界区，那么在有一个线程进入后其他所有试图访问此临界区的线程将被挂起，并一直持续到进入临界区的线程离开。临界区在被释放后，其他线程可以继续抢占，并以此达到用原子方式操作共享资源的目的。 
    临界区在用户模式下，不会发生用户态到内核态的切换

- 2、互斥量 Synchronized/Lock：

    采用互斥对象机制，只有拥有互斥对象的线程才有访问公共资源的权限。因为互斥对象只有一个，所以可以保证公共资源不会被多个线程同时访问

    功能上跟临界区类似，不过可用于不同进程间的线程同步。

- 3、信号量 Semphare：(????)

    为控制具有有限数量的用户资源而设计的，它允许多个线程在同一时刻去访问同一个资源，但一般需要限制同一时刻访问此资源的最大线程数目。

    信号量用于限制对临界资源的访问数量，保证了消费数量不会大于生产数量。

- 4、事件(信号)，Wait/Notify：(????)

    通过通知操作的方式来保持多线程同步，还可以方便的实现多线程优先级的比较操作

    触发重置事件对象，那么等待的所有线程中将只有一个线程能唤醒，并同时自动的将此事件对象设置为无信号的；它能够确保一个线程独占对一个资源的访问。和互斥量的区别在于多了一个前置条件判定。

    https://blog.csdn.net/gamekit/article/details/80788579

他们的主要区别在于：https://blog.csdn.net/cenchure/article/details/16991803

适用范围（能否跨进程）：临界区在用户模式下，不会发生用户态到内核态的切换，只能用于同进程内线程间同步。其他会导致用户态到- 内核态的切换，利用内核对象实现，可用于不同进程间的线程同步。

性能：临界区性能较好，一般只需数个CPU周期。其他机制性能相对较差，一般需要数十个CPU周期；临界区不支持等待时间，为了获取临界资源，需要不断轮询（死循环或Sleep一段时间后继续查询），其他机制内核负责触发，在对临界资源竞争较少的情况下临界区的性能表现较好，在对临界区资源竞争激烈的情况下临界区有额外的CPU损耗（死循环方式下）或响应时间延迟（Sleep方式下）。

应用范围:

 可用临界区机制实现同进程内的互斥量、事件、信号量功能；

互斥量实现了互斥使用临界资源；

事件实现单生产多消费（同时只能一个消费）功能；

信号量实现多生产多消费功能



#### 线程同步及系统调用

> 线程间的同步方式级对应的系统调用

- 信号量
    信号量是一种特殊的变量，可用于线程同步。它只取自然数值，并且只支持两种操作：

    P(SV)：如果信号量SV大于0，将它减一；如果SV值为0，则挂起该线程。

    V(SV)：如果有其他进程因为等待SV而挂起，则唤醒，然后将SV+1；否则直接将SV+1。

    其系统调用为：

    sem_wait（sem_t *sem）：以原子操作的方式将信号量减1，如果信号量值为0，则sem_wait将被阻塞，直到这个信号量具有非0值。

    sem_post（sem_t *sem)：以原子操作将信号量值+1。当信号量大于0时，其他正在调用sem_wait等待信号量的线程将被唤醒。

- 互斥量

    互斥量又称互斥锁，主要用于线程互斥，不能保证按序访问，可以和条件锁一起实现同步。当进入临界区      时，需要获得互斥锁并且加锁；当离开临界区时，需要对互斥锁解锁，以唤醒其他等待该互斥锁的线程。

    其主要的系统调用如下：

    pthread_mutex_init：初始化互斥锁

    pthread_mutex_destroy：销毁互斥锁

    pthread_mutex_lock：以原子操作的方式给一个互斥锁加锁，如果目标互斥锁已经被上锁，该函数调用将阻塞，直到该互斥锁的占有者将其解锁

    pthread_mutex_unlock：以一个原子操作的方式给一个互斥锁解锁。

- 条件变量

    条件变量，又称条件锁，用于在线程之间同步共享数据的值。条件变量提供一种线程间通信机制：当某个共享数据达到某个值时，唤醒等待这个共享数据的一个/多个线程。即，当某个共享变量等于某个值时，调用 signal/broadcast。此时操作共享变量时需要加锁。

    其主要的系统调用如下：

    pthread_cond_init：初始化条件变量

    pthread_cond_destroy：销毁条件变量

    pthread_cond_signal：唤醒一个等待目标条件变量的线程。哪个线程被唤醒取决于调度策略和优先级

    pthread_cond_wait：等待目标条件变量。需要一个加锁的互斥锁确保操作的原子性。该函数中在进入wait状态前首先进行解锁，然后接收到信号后会再加锁，保证该线程对共享资源正确访问

> [操作系统（一）](https://www.nowcoder.com/tutorial/93/156e55e0579d4a678e857b34d572c278)

#### 进程状态转换图

动态就绪，静态就绪，动态阻塞，静态阻塞

-----------------------------------------------------------------

### IO

#### 缓存 I/O

缓存 I/O 又被称作标准 I/O，大多数文件系统的默认 I/O 操作都是缓存 I/O。在 Linux 的缓存 I/O 机制中，操作系统会将 I/O 的数据缓存在文件系统的页缓存（ page cache ）中，也就是说，<font color=red>数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间</font>。

缓存 I/O 的缺点：

数据在传输过程中需要在应用程序地址空间和内核进行多次数据拷贝操作，这些数据拷贝操作所带来的 CPU 以及内存开销是非常大的。

#### IO模式

刚才说了，对于一次IO访问（以read举例），数据会先被拷贝到操作系统内核的缓冲区中，然后才会从操作系统内核的缓冲区拷贝到应用程序的地址空间。所以说，当一个read操作发生时，它会经历两个阶段：
1. <font color=red>等待数据准备 (Waiting for the data to be ready)</font>
2. <font color=red>将数据从内核拷贝到进程中 (Copying the data from the kernel to the process)</font>

正式因为这两个阶段，linux系统产生了下面五种网络模式的方案。
- 阻塞 I/O（blocking IO）
- 非阻塞 I/O（nonblocking IO）
- I/O 多路复用（ IO multiplexing）/事件驱动模型
- ~~信号驱动 I/O（ signal driven IO）~~
- 异步 I/O（asynchronous IO）

注：由于signal driven IO在实际中并不常用，所以我这只提及剩下的四种IO Model。

##### 阻塞 I/O（blocking IO，BIO）

在linux中，默认情况下所有的socket都是blocking，一个典型的读操作流程如图所示：

![img](assets/720333-20160916171008617-1558216223.png)

当用户进程调用了recvfrom这个系统调用，kernel就开始了IO的第一个阶段：准备数据（对于网络IO来说，很多时候数据在一开始还没有到达。比如，还没有收到一个完整的UDP包。这个时候kernel就要等待足够的数据到来）。这个过程需要等待，也就是说数据被拷贝到操作系统内核的缓冲区中是需要一个过程的。而在用户进程这边，整个进程会被阻塞（当然，是进程自己选择的阻塞）。当kernel一直等到数据准备好了，它就会将数据从kernel中拷贝到用户内存，然后kernel返回结果，用户进程才解除block的状态，重新运行起来。

所以，<font color=red>blocking IO的特点就是在IO执行的两个阶段都被block</font>。

--------------------------------------------------------------------------------------------

##### 非阻塞 I/O（nonblocking IO，NIO）

linux下，可以通过设置socket使其变为non-blocking。当对一个non-blocking socket执行读操作时，流程如图所示：

![img](assets/720333-20160916171226852-1916489268.png)

当用户进程发出read操作时，如果kernel中的数据还没有准备好，那么它并不会block用户进程，而是立刻返回一个error。从用户进程角度讲 ，它发起一个read操作后，并不需要等待，而是马上就得到了一个结果。用户进程判断结果是一个error时，它就知道数据还没有准备好，于是它可以再次发送read操作。一旦kernel中的数据准备好了，并且又再次收到了用户进程的system call，那么它马上就将数据拷贝到了用户内存，然后返回。

所以，<font color=red>nonblocking IO的特点是用户进程需要不断的主动询问kernel数据好了没有</font>。

-------------------------------------------------------------------------

##### I/O 多路复用（ IO multiplexing）

IO multiplexing又称为<font color=red>事件驱动模型，也就是select，poll，epoll</font>，有些地方也称这种IO方式为event driven IO。select/epoll的好处就在于单个process就可以同时处理多个网络连接的IO。

它的基本原理就是select、poll、epoll这个function会不断的轮询所负责的所有socket，当某个socket有数据到达了，就通知用户进程。流程如图所示：

![img](assets/720333-20160916171333523-650292614.png)

> 这个图和blocking IO的图其实并没有太大的不同，事实上，还更差一些。因为这里需要使用两个system call (select 和 recvfrom)，而blocking IO只调用了一个system call (recvfrom)。<font color=red>但是，用select的优势在于它可以同时处理多个connection</font>。
>
> 所以，如果处理的连接数不是很高的话，使用select/epoll的web server不一定比使用multi-threading + blocking IO的web server性能更好，可能延迟还更大。<font color=red>select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。</font>

在IO multiplexing Model中，实际中，对于每一个socket，一般都设置成为non-blocking，但是，如上图所示，整个用户的process其实是一直被block的。只不过process是被select这个函数block，而不是被socket IO给阻塞。

##### 异步IO

…

> [IO模式](https://www.cnblogs.com/zhangmingda/p/9396994.html)

---------------------------------------------------------

#### 同步异步/阻塞非阻塞

同步IO在进行IO操作时会阻塞进程；

异步IO将IO操作交给内核完成，然后内核做完后发信号通知；



简单点理解就是：

1. 同步，就是我调用一个功能，该功能没有结束前，我死等结果。
2. 异步，就是我调用一个功能，不需要知道该功能结果，该功能有结果后通知我（回调通知）。
3. 阻塞，就是调用我（函数），我（函数）没有接收完数据或者没有得到结果之前，我不会返回。
4. 非阻塞，就是调用我（函数），我（函数）立即返回，通过select通知调用者

同步IO和异步IO的区别就在于：<font color=red>数据拷贝的时候进程是否阻塞</font>

阻塞IO和非阻塞IO的区别就在于：<font color=red>应用程序的调用是否立即返回</font>

--------------------------------------------------------------------

#### 事件驱动模型的比较

IO multiplexing又称为<font color=red>事件驱动模型，也就是select，poll，epoll</font>，有些地方也称这种IO方式为event driven IO。<font color=red>select/epoll的优势并不是对于单个连接能处理得更快，而是在于能处理更多的连接。</font>

##### select

##### poll

##### epoll

 select(),poll()模型都是水平触发模式，信号驱动IO是边缘触发模式，epoll()模型即支持水平触发，也支持边缘触发，默认是水平触发。

poll与select的区别<sup>【3】</sup>：

- 最大连接数的限制

    - select有限制（32位默认1024/64位默认2048）
    - poll没有最大文件描述符数量的限制

- 描述fd集合的方式

    - select使用fd_set集合

        ```
        fd_set fds;
        ```

    - poll使用pollfd结构体

        ```
        struct pollfd{
        	...
        }
        
        struct pollfd fds[OPEN_MAX_NUM];
        ```

select、poll、epoll三者区别:

1. 支持一个进程所能打开的最大连接数
    - select：32位系统默认1024/64位系统默认2048
    - poll：没有最大连接数的限制，原因是它是基于链表来存储的
    - epoll：连接数有上限，但是很大，1G内存的机器上可以打开10万左右的连接
2. FD剧增后带来的IO效率问题——内核如何检测文件描述符是否可读可写
    - select：每次调用时都会对连接进行线性遍历，所以随着FD的增加会造成遍历速度慢的线性下降性能问题
    - poll：同上
    - epoll：通过内核和用户空间共享一块内存来实现的（mmap）
3. 消息传递方式——如何找到就绪的文件描述符传递给用户态
    - select：内核需要将消息传递到用户空间，都需要内核拷贝动作
    - poll：同上
    - epoll：通过内核和用户空间共享一块内存来实现的（mmap）





为什么epoll更加高效？

通过比较select、poll和epoll==处理I/O的过程==来剖析epoll比select和poll高效的主要原因：

1. 用户态将文件描述符传入内核的方式：

    - select：创建3个文件描述符集并拷贝到内核中,分别监听读、写、异常动作。这里受到单个进程可以打开的fd数量限制,默认是1024。；

    - poll：将传入的struct pollfd结构体数组拷贝到内核中进行监听；

        （select、poll都是拷贝到内核中）

    - epoll：执行epoll_create会在内核的高速cache区中建立<font color=red>一颗红黑树</font>以及<font color=red>就绪链表</font>(该链表存储已经就绪的文件描述符)。接着用户执行的epoll_ctl函数添加文件描述符会在红黑树上增加相应的结点；

2. 内核检测文件描述符是否可读可写的方式：

    - select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。 

    - poll：将之前传入的fd数组拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断。 

        （select、poll都是采用轮询的方式，遍历所有的fd）

    - epoll采用回调机制+就绪链表。在执行`epoll_ctl`的`add`操作时，不仅将文件描述符放到红黑树上，而且也注册了回调函数，内核在检测到某文件描述符中断到了时（可读/可写），会调用回调函数，该回调函数将文件描述符放在就绪链表中；

3. 如何找到就绪的文件描述符传递给用户态：

    - select：将之前传入的fd_set拷贝传出到用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态，需要遍历来判断； 

    - poll：将之前传入的<font color=red>fd结构体数组</font>拷贝传出用户态并返回就绪的文件描述符总数。用户态并不知道是哪些文件描述符处于就绪态,需要遍历来判断； 

    - epoll：epoll_wait只用观察就绪链表中有无数据即可，最后将链表的数据返回给数组并返回就绪的数量。内核将就绪的文件描述符放在传入的数组中,所以只用遍历依次处理即可。这里返回的文件描述符是通过mmap让内核和用户空间共享同一块内存实现传递的,减少了不必要的拷贝；

4. 继续重新监听时如何重复以上步骤：

    - select：将新的监听文件描述符集合拷贝传入内核中,继续以上步骤；

    - poll：将新的`struct pollfd`结构体数组拷贝传入内核中,继续以上步骤； 

    - epoll：无需重新构建红黑树，直接沿用已存在的即可，只需要做增、删修改即可；

    

> [【1】epoll比select和poll高效的原因](https://blog.csdn.net/Move_now/article/details/71773965)
>
> https://www.cnblogs.com/NerdWill/p/4996476.html
>
> [【3】select、poll、epoll区别](https://ww.cnblogs.com/aspirant/p/9166944.html)
>
> [【4】epoll的原理介绍](https://www.cnblogs.com/zhangzl/p/5195667.html)

最后总结，epoll比select和poll高效的根本原因主要有两点:

- 减少了用户态和内核态之间的文件描述符拷贝 

    - select、poll都需要将有关文件描述符的数据结构拷贝进内核,最后再拷贝出来；

    - epoll创建的有关文件描述符的数据结构本身就存于内核态中，系统调用返回时也采用mmap共享存储区，需要拷贝的次数大大减少；【存储映射I/O (Memory-mapped I/O)，mmap】

- 减少了对就绪文件描述符的遍历

    - select、poll采用轮询的方式来检查文件描述符是否处于就绪态;（导致随着fd的增加，效率会明显降低）

    - epoll采用回调机制；（IO效率不会随着FD数量增大而线性下降）

--------------------------